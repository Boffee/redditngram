{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2\n",
    "import datetime\n",
    "import itertools\n",
    "import lzma as xz\n",
    "import logging\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "\n",
    "from collections import Counter\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from functools import partial\n",
    "from multiprocessing import Pool\n",
    "from multiprocessing.dummy import Pool as dPool\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['REDDIT_DATA'] = \"/media/brian/ColdStore/Datasets/nlp/reddit\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "_REDDIT_COMMENT_BASE_URL = \"https://files.pushshift.io/reddit/comments/\"\n",
    "_BZ2_FILENAME_TEMPLATE = \"RC_%Y-%m.bz2\"\n",
    "_XZ_FILENAME_TEMPLATE = \"RC_%Y-%m.xz\"\n",
    "_DATA_START_DATE = datetime.date(2005, 12, 1)\n",
    "_XZ_START_DATE = datetime.date(2017, 12, 1)\n",
    "\n",
    "\n",
    "DEFAULT_REDDIT_DATA = os.environ.get('REDDIT_DATA') or os.path.expanduser(\"~/reddit\")\n",
    "DEFAULT_REDDIT_COMMENTS_DATA = os.path.join(DEFAULT_REDDIT_DATA, \"comments\")\n",
    "\n",
    "\n",
    "def populate_reddit_comments_json(dest=DEFAULT_REDDIT_COMMENTS_DATA):\n",
    "    curr_date = _DATA_START_DATE\n",
    "    end_date = datetime.date.today() + relativedelta(months=-1)\n",
    "    dates = []\n",
    "    while curr_date <= end_date:\n",
    "        dates.append(curr_date)\n",
    "        curr_date += relativedelta(months=1)\n",
    "    download_fn = partial(_download_reddit_comments_json, dest=dest)\n",
    "    # Using too many processes causes \"ERROR 429: Too Many Requests.\"\n",
    "    list(multiproc_imap(download_fn,\n",
    "                        dates,\n",
    "                        processes=4,\n",
    "                        thread_only=True,\n",
    "                        total=len(dates)))\n",
    "\n",
    "\n",
    "def download_reddit_comments_json(year, month, dest=DEFAULT_REDDIT_COMMENTS_DATA):\n",
    "    url = get_reddit_comments_url(year, month)\n",
    "    if not url:\n",
    "        logging.warning(datetime.date(year, month, 1).strftime(\"No data exists for %Y-%m.\"))\n",
    "        return False\n",
    "    return download(url, dest=dest)\n",
    "\n",
    "\n",
    "def load_reddit_comments_json(year, month, root=DEFAULT_REDDIT_COMMENTS_DATA):\n",
    "    path = get_reddit_comments_local(year, month)\n",
    "    if not path:\n",
    "        logging.warning(datetime.date(year, month, 1).strftime(\"No data exists for %Y-%m.\"))\n",
    "        return None\n",
    "    assert path.endswith('.bz2') or path.endswith('.xz'), (\n",
    "        \"Failed to load {}.Only bz2 and xz are supported.\".format(path))\n",
    "    reader = bz2.BZ2File if path.endswith('.bz2') else xz.LZMAFile\n",
    "    with reader(path, 'r') as fh:\n",
    "        for line in fh:\n",
    "            yield json.loads(line.decode())\n",
    "\n",
    "\n",
    "def _download_reddit_comments_json(date, dest=DEFAULT_REDDIT_COMMENTS_DATA):\n",
    "    return download_reddit_comments_json(date.year, date.month, dest=dest)\n",
    "\n",
    "\n",
    "def get_reddit_comments_url(year, month):\n",
    "    target_date = datetime.date(year, month, 1)\n",
    "    url = _get_reddit_comments_path(target_date, _REDDIT_COMMENT_BASE_URL)\n",
    "    return url\n",
    "    \n",
    "\n",
    "def get_reddit_comments_local(year, month, root=DEFAULT_REDDIT_COMMENTS_DATA):\n",
    "    target_date = datetime.date(year, month, 1)\n",
    "    path = _get_reddit_comments_path(target_date, root=root)\n",
    "    return path\n",
    "\n",
    "\n",
    "def download(url, dest='/tmp/'):\n",
    "    filename = os.path.basename(url)\n",
    "    if dest[-1] == '/' or os.path.isdir(dest):\n",
    "        if not os.path.isdir(dest):\n",
    "            os.makedirs(dest)\n",
    "        dest = os.path.join(dest, filename)\n",
    "    if os.path.isfile(dest):\n",
    "        logging.info(\"{} already exist in {}.\".format(url, dest))\n",
    "    else:\n",
    "        logging.info(\"Downloading {} to {}...\".format(url, dest))\n",
    "        resp = requests.get(url, stream=True)\n",
    "        if not resp.ok:        \n",
    "            logging.warning(\"{}: {}\".format(resp.reason, url))\n",
    "            return False\n",
    "        total_size = int(resp.headers.get('content-length', 0)); \n",
    "        block_size = 2**20\n",
    "        with open(dest, 'wb') as fh:\n",
    "            for data in tqdm(resp.iter_content(block_size),\n",
    "                             unit=\"MB\",\n",
    "                             total=total_size//block_size):\n",
    "                fh.write(data)\n",
    "    return True    \n",
    "\n",
    "\n",
    "def multiproc_imap(func,\n",
    "                   iterable,\n",
    "                   processes=None,\n",
    "                   thread_only=False,\n",
    "                   total=None,\n",
    "                   chunksize=1):\n",
    "    pool_fn = dPool if thread_only else Pool\n",
    "    pool = pool_fn(processes=processes)\n",
    "    return tqdm(pool.imap(func, iterable, chunksize=chunksize), total=total)\n",
    "\n",
    "\n",
    "def _get_reddit_comments_path(date, root):\n",
    "    if not _validate_reddit_comments_date(date):\n",
    "        return None\n",
    "    filename = _get_reddit_comments_filename(date)\n",
    "    path = os.path.join(root, filename)\n",
    "    return path\n",
    "\n",
    "\n",
    "def _get_reddit_comments_filename(date):\n",
    "    if date < _XZ_START_DATE:\n",
    "        return date.strftime(_BZ2_FILENAME_TEMPLATE)\n",
    "    else:\n",
    "        return date.strftime(_XZ_FILENAME_TEMPLATE)\n",
    "\n",
    "\n",
    "def _validate_reddit_comments_date(date):\n",
    "    start_date = _DATA_START_DATE\n",
    "    end_date = datetime.date.today() + relativedelta(months=-1)\n",
    "    if (date > end_date or date < start_date):\n",
    "        logging.warning(\"date must be between {} and {}: given {}\".format(\n",
    "            start_date.strftime(\"%Y-%m\"), \n",
    "            end_date.strftime(\"%Y-%m\"), \n",
    "            date.strftime(\"%Y-%m\")))\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Reddit Comments\n",
    "\n",
    "There's about 450GB of data from 2005-12 to 2018-09, so make sure you have enough disk space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9b8f7f88ae7425da7f1a16fad25adc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=155), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fb1306f9485474784ef552e63648b5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=7424), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "populate_reddit_comments_json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_TOKEN_MAX_CHARS = 25\n",
    "\n",
    "\n",
    "def extract_reddit_comments_upto_ngram_strs(year, month, n):\n",
    "    \"\"\"Extract 1- to n-gram simultaneously because file load is the bottleneck.\"\"\"\n",
    "    jsons = load_reddit_comments_json(year, month)\n",
    "    texts = map(lambda d: d['body'], jsons)\n",
    "    for text in texts:\n",
    "        upto_ngrams = []\n",
    "        for m in range(n):\n",
    "            mgrams = extract_filtered_ngram_strs(text, m)\n",
    "            upto_ngrams.append(mgrams)\n",
    "        yield upto_ngrams\n",
    "\n",
    "\n",
    "def extract_reddit_comments_ngram_strs(year, month, n):\n",
    "    jsons = load_reddit_comments_json(year, month)\n",
    "    texts = map(lambda d: d['body'], jsons)\n",
    "    ngram_strs = map(lambda s: extract_filtered_ngram_strs(s, n), texts)\n",
    "    return ngram_strs\n",
    "\n",
    "\n",
    "def extract_filtered_ngram_strs(text, n, tok_max_chars=DEFAULT_TOKEN_MAX_CHARS):\n",
    "    text_cleaned = re.sub('\\s+', ' ', text)\n",
    "    token_match_str = \"[\\^ ][^ ]{1,%d}\" % tok_max_chars\n",
    "    ngram_match_str = \"(?=(%s))\" % (token_match_str * n)\n",
    "    return re.findall(ngram_match_str, text_cleaned)\n",
    "\n",
    "\n",
    "def extract_filtered_ngram_strs_slow(text, n, tok_max_chars=DEFAULT_TOKEN_MAX_CHARS):\n",
    "    ngrams = extract_ngrams(text, n)\n",
    "    filtered_ngrams = filter(\n",
    "        lambda ngram: not has_long_token(ngram, tok_max_chars=tok_max_chars), ngrams)\n",
    "    filtered_ngram_strs = map(\n",
    "        lambda ngram: ' '.join(ngram), filtered_ngrams)\n",
    "    return filtered_ngram_strs\n",
    "\n",
    "\n",
    "def extract_ngrams(text, n):\n",
    "    tokens = text.split()\n",
    "    return zip(*[tokens[i:] for i in range(n)])\n",
    "\n",
    "\n",
    "def has_long_token(tokens, tok_max_chars=DEFAULT_TOKEN_MAX_CHARS):\n",
    "    for tok in tokens:\n",
    "        if len(tok) > tok_max_chars:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Benchmark n-gram extraction with regex vs tokenization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_string = \"asdf \" * 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%timeit\n",
    "list(extract_filtered_ngram_strs(test_string, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "list(extract_filtered_ngram_strs_slow(test_string, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Benchmark file loading vs file loading + n-gram extraction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%timeit -r 1 -n 1\n",
    "_ = list(load_reddit_comments_json(2006, 12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -r 1 -n 1\n",
    "_ = list(extract_reddit_comments_ngram_strs(2006, 12, 3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
